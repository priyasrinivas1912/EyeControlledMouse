# EyeControlledMouse

The Eye-Controlled Mouse Project is an innovative system that enables hands-free computer interaction by tracking users' eye movements and translating them into cursor actions. Utilizing computer vision and eye-tracking technology, the system detects facial landmarks, monitors eye movement, and maps gaze positions to screen coordinates. Additionally, it incorporates blink detection or gaze-dwell techniques for performing mouse clicks. The project leverages OpenCV for real-time image processing, MediaPipe for facial landmark detection, and PyAutoGUI for GUI automation, ensuring seamless and efficient operation.

ğŸ” Current (Existing) Eye-Controlled Mouse Systems: An Overview
1. Core Components

Most systems follow this block architecture:

+---------------------+
|     Webcam Input    |
+----------+----------+
           |
+----------v----------+
| Frame Preprocessing |
+----------+----------+
           |
   +-------v--------+
   | Eye/Face Tracker|
   +-------+--------+
           |
   +-------v--------+
   | Gaze Estimation |
   +-------+--------+
           |
   +-------v--------+
   | Cursor Control  |
   +-----------------+

ğŸ§  How It Works (Technically)

Step 1: Camera Initialization

â€œWe begin by activating the webcam to capture live facial video.
Each frame is instantly processed to detect a faceâ€”this forms the foundation of our systemâ€™s real-time responsiveness.â€

Step 2: Face & Eye Landmark Detection

â€œNext, using MediaPipe Face Mesh, a cutting-edge AI framework, we detect precise landmarks on the face and eyes.
These key points are like a digital map of your eye movementsâ€”this is where innovation happens, because we convert raw video into actionable signals.â€

Step 3: Eye Movement Tracking

â€œFinally, we extract the pupil coordinates to detect exactly where youâ€™re looking and translate that into on-screen cursor movement.
This means your gaze becomes a powerful input device.â€

Step 4: Cursor Movement Execution

â€œAfter detecting where the user is looking, we use PyAutoGUI, a powerful Python automation library, to actually move the mouse pointer.
So when the system detects your gaze shifting left, right, up, or downâ€”your cursor follows seamlessly in real time.
This is the moment the eyes truly become the mouse.â€

âš¡ Step 5: Blink Detection for Clicks

â€œWe needed a natural way to click without using hands.
So, we designed a mechanism to detect intentional blinks using threshold values.
A blink is no longer just a reflexâ€”it becomes a command. A blink triggers a click.
This unlocks hands-free control for people with mobility limitations or scenarios where touch is impractical.â€
ğŸ§° Tools and Libraries Commonly Used

OpenCV â€“ For camera I/O and image processing.

Dlib / Mediapipe â€“ For face and landmark detection.

PyAutoGUI / pynput â€“ To control the mouse.
